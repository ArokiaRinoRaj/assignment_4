{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779b6bd-8124-4fa5-ad35-84118253656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f430b-a467-4707-ad2d-425baa268598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 1: Load CSV =====\n",
    "df = pd.read_csv(\"C:/Users/dell/Desktop/project/Swiggy/swiggy.csv\")\n",
    "print(f\"Original shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368398c-5d86-4057-b15c-6b9d0c23fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 2: Drop duplicates =====\n",
    "key_cols = ['name', 'city', 'cuisine','address']\n",
    "\n",
    "# Find all duplicate rows for these columns (including the first occurrence)\n",
    "duplicates = df[df.duplicated(subset=key_cols, keep=False)]\n",
    "\n",
    "# Sort\n",
    "duplicates = duplicates.sort_values(by=key_cols)\n",
    "\n",
    "# Display\n",
    "print(f\"Number of duplicate rows (based on {key_cols}): {duplicates.shape[0]}\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59594c41-0d88-465d-b751-ec8b1949d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=key_cols, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730962f8-5b3b-424e-b147-e92e99fcc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed041de5-367b-46b9-b3d4-b315a08d443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['rating']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd848b-70dc-427a-9b43-7dd5da2f205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 3: Clean numeric fields =====\n",
    "# Clean 'rating'  replace '--' with NaN\n",
    "df['rating'] = df['rating'].replace(['--'], np.nan)\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14270bb9-9f27-4bcd-9bc1-e37c33d1adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['rating_count']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd1961-4c6b-4932-a31d-fe0978996a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'rating_count' replace 'Too Few Ratings' with NaN\n",
    "df['rating_count'] = df['rating_count'].replace('Too Few Ratings', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c3354-f795-4b67-a136-4465e920c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'rating_count' - extract numeric ('50K+ ratings' to 50000)\n",
    "\n",
    "def clean_rating_count(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = val.split()[0]\n",
    "    val_str = val_str.replace('+', '') \n",
    "    \n",
    "    if 'K' in val_str.upper():\n",
    "        number = val_str.upper().replace('K', '')\n",
    "        return int(float(number) * 1000)\n",
    "    elif val_str.isdigit():\n",
    "        return int(val_str)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['rating_count'] = df['rating_count'].apply(clean_rating_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a542af-cccc-4808-bcc3-6d07c7c7a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['rating_count']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158d16c-6093-40dd-aff5-5ed2e6d9129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['cost']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa317b5-976b-445e-abf7-39d92d3ca097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'cost' remove 'Rupees symbol'\n",
    "def clean_cost(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    val_str = str(val).strip()\n",
    "    # Remove currency symbol and commas\n",
    "    val_str = val_str.replace('â‚¹', '').replace(',', '')\n",
    "    # Take only the first number part\n",
    "    first_part = val_str.split()[0]\n",
    "    # Convert to float if it's numeric\n",
    "    return float(first_part) if first_part.replace('.', '', 1).isdigit() else None\n",
    "\n",
    "df['cost'] = df['cost'].apply(clean_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96e2ac-1824-45d8-b410-50eaf38c001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['rating', 'rating_count', 'cost']\n",
    "null_counts = df[numeric_cols].isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1defc-03a3-4f56-b5ce-718d26fa48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].fillna(df['rating'].median())\n",
    "df['rating_count'] = df['rating_count'].fillna(df['rating_count'].median())\n",
    "df['cost'] = df['cost'].fillna(df['cost'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b5f3f-283d-46a8-920b-a2004f3d8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts_all = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8f320-4e87-4ab8-84af-5ae06ce47e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25395f-33c7-4a46-abe5-f157a7eb0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['name', 'cuisine', 'address'])\n",
    "df['lic_no'] = df['lic_no'].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c77dfe-e8b8-4588-8a49-a2bf8a8b12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['city']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb005ad-296d-4ae4-a020-9b4c8aa8abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city_parts_count'] = df['city'].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "# See value counts\n",
    "print(df['city_parts_count'].value_counts())\n",
    "\n",
    "# Optional: Check rows that don't have exactly 2 parts\n",
    "invalid_rows = df[df['city_parts_count'] != 2]\n",
    "print(invalid_rows[['city']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bd0db-8759-4e31-892c-4d74d1d97f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split city into locality and city_main\n",
    "def split_city(val):\n",
    "    parts = str(val).split(',')\n",
    "    if len(parts) == 2:  # locality, city\n",
    "        return parts[0].strip(), parts[1].strip()\n",
    "    elif len(parts) == 1:  # only city\n",
    "        return \"Unknown\", parts[0].strip()\n",
    "    elif len(parts) >= 3:  # locality, sub-locality, city\n",
    "        return parts[0].strip(), parts[-1].strip()\n",
    "    else:\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "\n",
    "# Apply the function to each row\n",
    "df[['locality', 'city_main']] = df['city'].apply(lambda x: pd.Series(split_city(x)))\n",
    "\n",
    "# Drop city_parts_count\n",
    "df.drop(columns=['city_parts_count'], errors='ignore', inplace=True)\n",
    "\n",
    "print(df[['city', 'locality', 'city_main']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f87dc-c74d-410b-94c8-eae41a9e0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['city_main'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a9a58-eeab-47ff-8614-c88ee6ce6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c6bb2-35e2-40fc-906b-d1c2f3417ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['menu', 'link', 'lic_no', 'city'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030fbcf-001a-4372-bb88-ba0ffdab5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31d05d-3092-42c9-aefd-6b0fe1ab2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72dc21-62dc-4953-a479-cd52a9bff52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['name_encoded'] = label_encoder.fit_transform(df['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc230b-01ae-426e-a23f-597776f1f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  One-Hot Encode city_main \n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe_encoded = ohe.fit_transform(df[['city_main']])\n",
    "ohe_columns = ohe.get_feature_names_out(['city_main'])\n",
    "ohe_df = pd.DataFrame(ohe_encoded, columns=ohe_columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea004e-a9d3-4866-8898-8a7e557b91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords that indicate it's not a cuisine\n",
    "exclude_keywords = [\n",
    "    \"offer\", \"discount\", \"code\", \"free delivery\", \"default\", \"combo\",\n",
    "    \"popular brand store\", \"limited stocks\", \"use\", \"bill over\"\n",
    "]\n",
    "\n",
    "def is_valid_cuisine(cuisine):\n",
    "    cuisine_lower = cuisine.lower()\n",
    "    return not any(kw in cuisine_lower for kw in exclude_keywords)\n",
    "\n",
    "# Filter the list\n",
    "unique_cuisines = [c.strip() for c in unique_cuisines if is_valid_cuisine(c)]\n",
    "\n",
    "# Normalize capitalisation\n",
    "unique_cuisines = sorted(set(c.title() for c in unique_cuisines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83793d3e-20d7-4b83-90ab-5ffa9a18e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize capitalisation\n",
    "unique_cuisines = sorted(set(c.title() for c in unique_cuisines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e4a19-fea0-4086-94d0-c3395a845c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102455b-5138-4934-935c-4a2a5ebeeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split cuisines into lists in the DataFrame\n",
    "df['cuisine_list'] = df['cuisine'].str.split(',')\n",
    "\n",
    "# Multi-hot encode in one go\n",
    "cuisine_df = pd.DataFrame({\n",
    "    f'cuisine_{c}': df['cuisine_list'].apply(lambda x: int(c in [i.strip().title() for i in x]))\n",
    "    for c in unique_cuisines\n",
    "})\n",
    "\n",
    "# Merge with main DataFrame\n",
    "df = pd.concat([df, cuisine_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e855e-a43c-4767-9128-0bba11995b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode city_main\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "city_encoded = ohe.fit_transform(df[['city_main']])\n",
    "\n",
    "# Add to DataFrame\n",
    "city_encoded_df = pd.DataFrame(city_encoded, columns=ohe.get_feature_names_out(['city_main']), index=df.index)\n",
    "\n",
    "# Merge and drop original city_main\n",
    "df = pd.concat([df.drop(columns=['city_main']), city_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30875f0-67c1-4ed5-a3e2-cf09b814e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-model columns if needed\n",
    "model_df = df.drop(columns=['address', 'cuisine', 'cuisine_list','name', 'locality'], errors='ignore')\n",
    "\n",
    "model_df.to_csv(\"encoded_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862761bf-330e-49b6-94e3-41cdf9620fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder.pkl\n",
    "encoders = {\n",
    "    'name_encoder': label_encoder,\n",
    "    'city_main_encoder': ohe,\n",
    "    'unique_cuisines': unique_cuisines\n",
    "}\n",
    "\n",
    "# Save all in one pickle file\n",
    "with open(\"encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoders, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4fde55-723c-455a-8951-01e60d9edc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load data\n",
    "df_encoded = pd.read_csv(\"encoded_data.csv\")\n",
    "df_cleaned = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "# Compute similarity matrix (features only, drop id)\n",
    "X = df_encoded.drop(columns=['id'])\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "def get_similar_restaurants(restaurant_id, top_n=5):\n",
    "    # Find index of the restaurant\n",
    "    matches = df_encoded.index[df_encoded['id'] == restaurant_id]\n",
    "    if matches.empty:\n",
    "        return f\"No restaurant found with ID {restaurant_id}\"\n",
    "    \n",
    "    idx = matches[0]\n",
    "    \n",
    "    # Get the city of the input restaurant\n",
    "    input_city = df_cleaned.loc[df_cleaned['id'] == restaurant_id, 'city_main'].values[0]\n",
    "    \n",
    "    # Get similarity scores\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    \n",
    "    # Sort by similarity (highest first) and skip the restaurant itself\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:]\n",
    "    \n",
    "    # Filter results to the same city\n",
    "    same_city_scores = [s for s in sim_scores if df_cleaned.iloc[s[0]]['city_main'] == input_city]\n",
    "    \n",
    "    # Take top N\n",
    "    top_matches = same_city_scores[:top_n]\n",
    "    \n",
    "    # Get full details from cleaned data\n",
    "    similar_restaurants = df_cleaned.iloc[[i[0] for i in top_matches]]\n",
    "    \n",
    "    return similar_restaurants[['id', 'name', 'rating', 'rating_count', 'cost', 'cuisine', 'address', 'city_main']]\n",
    "\n",
    "# Example usage\n",
    "print(get_similar_restaurants(567335, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d36c7-a9e2-4d89-9b13-4a516aa0a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle\n",
    "\n",
    "# Load df_encoded in chunks\n",
    "def load_encoded(file_path, chunk_size=100_000):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # convert float64 -> float32 to save memory\n",
    "        float_cols = chunk.select_dtypes('float64').columns\n",
    "        chunk[float_cols] = chunk[float_cols].astype('float32')\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(\"Loading encoded data...\")\n",
    "df_encoded = load_encoded(\"encoded_data.csv\")\n",
    "\n",
    "# Features for clustering\n",
    "X = df_encoded.drop(columns=['id'])\n",
    "\n",
    "print(\"Fitting MiniBatchKMeans...\")\n",
    "kmeans = MiniBatchKMeans(n_clusters=10, random_state=42, batch_size=1000)\n",
    "df_encoded[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "# Save model\n",
    "with open(\"kmeans_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "# Save encoded data with clusters\n",
    "df_encoded.to_csv(\"encoded_with_clusters.csv\", index=False)\n",
    "print(\"Saved encoded_with_clusters.csv with cluster assignments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c5c24-f06a-4847-b2a5-225ec05e1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_recommendations(restaurant_id, top_n=5):\n",
    "    # Load df_cleaned on demand\n",
    "    df_cleaned = pd.read_csv(\"cleaned_data.csv\", dtype={'id':'int32', 'city_main':'category'})\n",
    "    \n",
    "    # Find the restaurant cluster\n",
    "    matches = df_encoded.index[df_encoded['id'] == restaurant_id]\n",
    "    if matches.empty:\n",
    "        return f\"No restaurant found with ID {restaurant_id}\"\n",
    "    idx = matches[0]\n",
    "    \n",
    "    cluster_id = df_encoded.loc[idx, 'cluster']\n",
    "    input_city = df_cleaned.loc[df_cleaned['id'] == restaurant_id, 'city_main'].values[0]\n",
    "    \n",
    "    cluster_restaurants = df_encoded[df_encoded['cluster'] == cluster_id]\n",
    "    same_city_ids = cluster_restaurants['id'].values\n",
    "    \n",
    "    filtered = df_cleaned[df_cleaned['id'].isin(same_city_ids)]\n",
    "    filtered = filtered[filtered['city_main'] == input_city]\n",
    "    filtered = filtered[filtered['id'] != restaurant_id]\n",
    "    \n",
    "    return filtered[['id','name','rating','rating_count','cost','cuisine','address','city_main']].sort_values(\n",
    "        by='rating', ascending=False\n",
    "    ).head(top_n)\n",
    "\n",
    "# Example usage\n",
    "print(get_cluster_recommendations(156602, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e74b2-d7b7-41f7-9f8e-7fbcbd56709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_recommendations(restaurant_id, top_n=5):\n",
    "    # Load df_cleaned on demand\n",
    "    df_cleaned = pd.read_csv(\"cleaned_data.csv\", dtype={'id':'int32', 'city_main':'category'})\n",
    "    \n",
    "    # Find the restaurant cluster\n",
    "    matches = df_encoded.index[df_encoded['id'] == restaurant_id]\n",
    "    if matches.empty:\n",
    "        return f\"No restaurant found with ID {restaurant_id}\"\n",
    "    idx = matches[0]\n",
    "    \n",
    "    cluster_id = df_encoded.loc[idx, 'cluster']\n",
    "    input_cuisine = df_cleaned.loc[df_cleaned['id'] == restaurant_id, 'cuisine'].values[0]\n",
    "    \n",
    "    cluster_restaurants = df_encoded[df_encoded['cluster'] == cluster_id]\n",
    "    same_cuisine_ids = cluster_restaurants['id'].values\n",
    "    \n",
    "    filtered = df_cleaned[df_cleaned['id'].isin(same_cuisine_ids)]\n",
    "    filtered = filtered[filtered['cuisine'] == input_cuisine]\n",
    "    filtered = filtered[filtered['id'] != restaurant_id]\n",
    "    \n",
    "    return filtered[['id','name','rating','rating_count','cost','cuisine','address','city_main']].sort_values(\n",
    "        by='rating', ascending=False\n",
    "    ).head(top_n)\n",
    "\n",
    "# Example usage\n",
    "print(get_cluster_recommendations(156602, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb671836-75db-4ff8-a26b-e8e7379d48f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
